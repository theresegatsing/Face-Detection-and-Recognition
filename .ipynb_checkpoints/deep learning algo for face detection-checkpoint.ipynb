{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f625462d-8d06-43d1-91a8-10a0514f8185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import os\n",
    "from PIL import Image\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "567237b3-3ab1-4ff3-8fa6-cfbbda985afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"dataset\", img_size=(64, 64)):\n",
    "    sets = [\"train\", \"test\"]\n",
    "    classes = [\"no_face\", \"face\"]\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for s in sets:\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for label, cls in enumerate(classes):\n",
    "            class_dir = os.path.join(data_dir, s, cls)\n",
    "\n",
    "            for file in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, file)\n",
    "\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert(\"RGB\")\n",
    "                    img = img.resize(img_size)\n",
    "                    X.append(np.array(img))\n",
    "                    y.append(label)\n",
    "                except:\n",
    "                    continue  # skip corrupted images\n",
    "\n",
    "        data[s] = (np.array(X), np.array(y))\n",
    "\n",
    "    train_x_orig, train_y_orig = data[\"train\"]\n",
    "    test_x_orig, test_y_orig = data[\"test\"]\n",
    "    \n",
    "    # Flatten images\n",
    "    train_x = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "    test_x  = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "    \n",
    "    # Normalize\n",
    "    train_x = train_x.astype(np.float32) / 255.0\n",
    "    test_x  = test_x.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Labels: reshape ONLY to (1, m)\n",
    "    train_y = train_y_orig.reshape(1, -1)\n",
    "    test_y  = test_y_orig.reshape(1, -1)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, np.array(classes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c8484b2-e180-4ef0-9328-b50657d9c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07b92fd8-a2d7-4cd7-b282-45f0ce9b7578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 0 → no_face\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQJVJREFUeJztnWmQXPd13e97vc70LD37DDDYBzsIgNgIkCAJUCQtUrZEapdcMS1HtsuOLKeSsuPkg1OVRdYHV2ylIieyoiWSLUYrKZHiToIkiI3YdwwwwAwGmH3rWXvvlw92/WPnnhvPCAAJiuf38eCP12/tO6/u6XO9IAgCIYQQQkTEf7d3gBBCyO0DiwIhhBAHiwIhhBAHiwIhhBAHiwIhhBAHiwIhhBAHiwIhhBAHiwIhhBBHeLYL3++/cQuCIhA9uNbzC8ZGsD6VzeqlJbztsrJyqIfE2hesB+DSe+/vS/xLCL72IvhCl4r6Piya93IUyhE/MpsdI+8SnmfdE/8XvikQQghxsCgQQghxsCgQQghxsCgQQghxsCgQQghxzNp99H7HA06jYlDCi5FTSURmhrug3t/7ktImZ7QTRERk3R2fxp9ZPh/KoQC7QXzgJjPMVOR9QhE4U/wAu9184x63nE22E4rcbvBNgRBCiINFgRBCiINFgRBCiINFgRBCiINFgRBCiMMLZhlq9H7PPvKAeyI7MwzXHt7/fah3nnwT6k0NaaX1Do7BtbXlq6C++zP/EuoVtWuhLu/z6/n+YG7ZRwXgKBofuALXllfWQT2ewLoHPnM2OTw3wtxu8ffH88DsI0IIIXOCRYEQQoiDRYEQQoiDRYEQQojjfRtzEZRyUC8Z/aZspk9p599+Gq7tPLcX6oODuqEsIlKR0LV58fIauPaZp96G+mT4f0L945/9E6j7FbVKCxtNKC/AfzsEjC64zcE3s2dMU/I9PVDn6qEn4dold30C6mUJfN8G4B4qlXBMTKGQh3rHxctQr6ysgHrrwqTeDwnBtZ7EsG5kvwQhvI8S6O28154SvikQQghxsCgQQghxsCgQQghxsCgQQghxsCgQQghxvG/dR6Ugiv8hGIXy6UPPKe3cPhxb0dE1DvW187W7Q0RkJKXdIIfPY89CTRV2WvzoyRegvmPH/VBfuv4xpQWG9YrDd365sAIdglxGaaEMdtnEq+fhjRjOpq7rF5T2w+89A9d2dmKX0WhqAup3bV4I9Q/u0pEb4QR2Hy1p+xTUI+EmqJt/T6Pjf489QHxTIIQQ4mBRIIQQ4mBRIIQQ4mBRIIQQ4mBRIIQQ4vildx9ZQyX88AzU39zzFNRffellpc2vSsC1n/sXX4B6cewbUN9zJKW0KwN64ImISD6N85OuYWOGvP7GHqjPX7pRabGKxXAt3hMRn4N63qMYbpiSvtLllfjvxlDpKtTPnExB/dvf0/fhsYP74NqxcTy8aiaPn9lt67BDarJH55VF6vCD0hXH+pKFvwv1kDRAHbuv6D4ihBDyHoVFgRBCiINFgRBCiINFgRBCiINFgRBCiOM96j7SHf6S0eEvTuMso/YTL0H9y1/6GtTjgXYnfPbfPA7XZqdPQf18Hz7dK9uWK23jFu2cEBH5ybNQlp5RnfMiIvK1b/8c6rW185X2Kx/CxxNP6v0TsV1JITNd573lwvhlxTKNoQF72SieUHj0xf8O9Rf3DUH97AX9oTmZgmv9EnbY7d6yBOqLq/DftmU57UoqTcXh2vzEGagPjWD3XmMTnjwngZ4m51kT8IznwXuX/1bnmwIhhBAHiwIhhBAHiwIhhBAHiwIhhBDHe7LRHIDGjefh+tbVfhDqnW88CfVouAzqreW6gXTgWCdcu3gN3saOh/4Y6iODbyvthR/gZvXWtS1Q77iK276Xu6qh/uT3dZxHrOc0XFu/+QNQ37jr01APQhGo4/krbD6/4xiN5kuXe5T2/BEc/3DkdfxcpabxEKhsuFxpw/1jcO2yphjUH16G9eZebMrwqtYpLblaayIiM8NvQD1TdgDq+cq7oB6P6WZ4wccnPGR1/N/lR4JvCoQQQhwsCoQQQhwsCoQQQhwsCoQQQhwsCoQQQhy3ufvI+hm4pjCDB3Mc3/cC1Pe8fQ1/ZC4E5UXLGpW2/zQeNLJ6s14rIpKMYIfQ0Q7twrh0Dbs4znYOQj0oGj/1L8M/6z/XryMGrmSxa2qm/TzU1++YhrpfjiM3rJ/7k1uDNWCquxvft//1L7+qtM7uK3DtyBC+r6an8f1Z15hUWrQMu9TOXMXRNP/qr/G2H9mMHXm7pENpv/bwp+Da7/38ENRrl16Hul/xM6hHvTuV1tSCnUqB4UryAvwcvlPwTYEQQoiDRYEQQoiDRYEQQoiDRYEQQoiDRYEQQojjtnYfWdEgPuja73/tp3gbA5eg3j6os4xERAZS2FETqdROiXV1eAezk5NQf+aZr0O9PKozXT73q81w7dkBvO1XXseOjbIodlMFWe1MeeNiCq79q7/4U6iH4tiB4hkXzgv0ZwY4EIncBPJ5PCDnZ888A/XBEe3gS2UycO1EGg/IGR/FeUb3P7BFaQcPHodrN6zFeV2xML5Xtty3EurxIKu0Z//mz+HayRG87Ym+XqjfueYE1P2k3nevtA2uDTyc5cTsI0IIIbcNLAqEEEIcLAqEEEIcLAqEEEIcLAqEEEIct4f7yBpA5Oehfv68npL09a9/C65dlDBa+Xmsl4pYv9wxpLRd61bAtT96GecqPfaRh6G+eaPWf/j1/wDXltdgR8kywwnVnzIcQlHtzLh4+ixc+9Z+nAvz2Ic/BvXAcB/RaXRjlErYMRcKYYfZoUN4OtoLL74M9XQ2rbTRAZz983uf1G4iEZHi1Fqoz1QnlTYwPALXPrihDeqffrQS6qXcDNTXffTLSnv2u38F1/pX8KTDRS3YBXj9BHYlLdmm85YKBf3dISISjsyHugi+zpib/3c93xQIIYQ4WBQIIYQ4WBQIIYQ4WBQIIYQ4WBQIIYQ4bg/3kVGaJkawi+eFHz2ltI4+7EC4HsbOjOHpAtSLJezuaaioUtqZPuyOWmo4nhZWaHeHiMi1nnal9c0k4doLx7EbJCwJqMd9nOVU3dSgtLa2VXDtntf2QP3XPvQY1P2QcUFpProhrElqFk8/jfPARobwBLMImAzoF43srEnsHHpkURLqT+7T0/siRnbWj9/A+zc0moL67316B9RLWX0fXurFeUPffeY01O/asATqj2TxZMSG1fr5DA3hrKmmeU9A3TOe5XfqAeKbAiGEEAeLAiGEEAeLAiGEEAeLAiGEEMc73GjGjbJiHg+O2f/Kk1A/vE//fL/ciACYyhmDcEq4MW0lMVwf003linY9lEREZNdHlkP98Dk88CdVuKq09ctwQ2zz8hqoH+sqg3rkSh3U0/kBpVXg2SYyPYEHrQwO4p/vtxjRAAE7zXNAPysTE+Nw5c9+9jOoHz12DOqT43gQzr33bFBaLsDmiLGSNl6IiDx7St9XIiI/OXpFaZGw0axtwvdJ9xC+D6citVA/seerStte0w/Xtu/EJotX3uyC+sYFi6Euh/R32dp79sOliUocFVJdtR3qgWgjwK2AbwqEEEIcLAqEEEIcLAqEEEIcLAqEEEIcLAqEEEIct859FGi3judF4NKLx5+Den/XUag/sk1HNMSxkUG+8lQP1BMJw61jRDSMjWsXxsb1LXDt/nbswGjbsBjqu3d+UmneJB7ice7Mj6C+uglHbnScx3EeQ6MTej+a74RrD3WfgHrXVe2aErHdR+9vDOdVgB15QaCv29e+9j/g2oMHD+NtF/BnFvEtIY1JHTuxoakcrt2xAD8nX/rvOIYlU4gqLexhN81nPojde9tacQRNqTgK9bGidiW9+DoeJFVtfIGE8VeWjKSwK2v4iHYkxuqxra+m9nWoV1TeAXXP05EjnmCX4o3ANwVCCCEOFgVCCCEOFgVCCCEOFgVCCCEOFgVCCCGOW+Y+yvvabZAexkNzzh16BepjKZxp0rZE55Q8vQc7mMZGslCvqsOOgESl3m8RkalJ7TY424e3vTCK9YdW4YElNdXahXHo6hRc+72XsXWkOpaCej6UhPrUpHZynD97Ga6tTWKn1uXLF6C+Y/tdUH9/g11GnvFn2ZXLeujL5U49qEZEJGQ8xTNTKaivX4PzsEohnQd27Ax2E529gh0/Q0Vs1/GjwH1Uhnf82Rfx98T8xxdAff2qlVAPF9qU9tyFH8C1mckU1Mtj2N1zaQDnMP3Gw9p5d+SEdvqJiKxa1w31ZPoi1BOJdVoMrDyxuQ1k+ofwTYEQQoiDRYEQQoiDRYEQQoiDRYEQQoiDRYEQQojjJriPDFdFSbtkrrY/A9d2XsCulzsffALq/Vk9PWkmhd06C+Zj90C/sb6mFucZlQJ9qjqu4ClODz6+Ceo/3o8nXi288n2ljQ7hXJjHtxvuqBk8CevaBHZsZECMzNlzXfgzP4KP51oPvm4lwxDxi/sh3vt45tHjzKo33nxdaRMpPKFwYgJPEWxbgqf0/cnncMbVM69o18/YhM5DEhEJ+9hhV4pih50n2q3jewm49vIYzhX66kvYrfOf7sHP+NQZ7Uj8yud2wLV/9hSeUtfTPw31w53YffTAgN7H+9cugWvzvfg7aLoOu4/Ky9cqzbsFf9fzTYEQQoiDRYEQQoiDRYEQQoiDRYEQQohjDo1mHK9QNBpo0wPtSuvtPgXXpsbHoe5l8PCQqlSf0j7+IPgJuIj8j2fwT+arqnB0Q1k5/pk+ap42N+AYjuvT+Gft4zO4Bt939wal3b3zQbi27/BfQj1agZtz/XtxM7jc103I2lbcIY7gXqN0n78E9d4ePCBofus8LZo/038vUFKKdTTFQK8VEcmk8b3y0otvKW0cDHoSEclkjKZvNV7fcx7Hkxw80KG0Yhzfs4kIbmJnMvg4JdDOhkgaD835nU+vgXoohE0WMcH66XH99XZ2H/5OiWbwNoICbpxnQvh74luv6/iPP10M7nsRyXXiCJGq5fh7Ml/Qho9QWEd5iIh4JWxUCVmZKP8AvikQQghxsCgQQghxsCgQQghxsCgQQghxsCgQQghxzNp9VMxjJ8PVjiNQv9yhu/ypLvzz7YdW45+79/Vil8Tw9GKl7d+P16ZSOEagsgG7DcrL8b4satWDSUZSOF5g/37sePrC7yyEekOVdjgsWJCEa195vQnqe1/E59Yz3AZ+XP98f+v25XgbHj6HDVXYkXbw4ItQ//jHP6e0wIx/eDdcSXPblyJw5PkBPleTY9pNJCLy+pvYaXLyhL6e1TU4ymTTOuxu2boGxz98/03sSDvWrd1A5cbQqUgUu2/yReyEEmCGmQ5jp1JfL46PeeQOfPyN5dgFmIrpgTffeDsF19YmjOMEw8JERGor8PdE66JFSjvTjeNJdjz0SagfP4njPEIx7bpsbcbfKXnjXsZeqn8M3xQIIYQ4WBQIIYQ4WBQIIYQ4WBQIIYQ4WBQIIYQ4Zu0+mpnAA2KGrnwX6pdOdSmt3K+Aa48OYRdP/TqdCSQi8ujjn1FaqvdP4NrVy6qh/tzRTqhPT1RCvbmxQX/mlM6KERFpW4gdCxND2sEkIvJi/0GlbR3G+UFtS7H76Mwp7UwQERmbwX6DxYsbldZ3FTu47r0T50St2JKC+unLp6Gez2oLSiSG/y55L0Qi+Z52eBx5DQ+SOn7qB1A/dR47uOoadODUyOgQXNsybwXUu88MQv210/hekTJ9fXwfu1hiUXyPZ3P4eEolvZ1YCX/9fHfPMNQTNdrZIyIS7RmA+oqoPs7f//BmuPbbr52BuvVXcy6Nh+/ctWGn0mI+dlNdaNc5SSIi59vxAJ8W0YOAqrbj74NoDR7sMxv7Ed8UCCGEOFgUCCGEOFgUCCGEOFgUCCGEOFgUCCGEOGbtPsrlsSNgaARnvVzu0FlJyVrsPorF8W5sWYOzeC4d/JHSli3BbohnDmJnUymHc1Rqa/A+Do2B4/dxTY3FW6D+/FHskti5U7sq6hfdA9dm+nGu0m/cjd0Q6WqcjbJknXYtZCfwJKzcAM7KyWRw/s2VLjyR7VqvnjS1dAl2lLwb2UfWJyKXkYhI++FDSvvhN78G117E5j2pamiFeiyin5/58/H5HhzHG//py+egHg7hqVw+yPnxfWxXCYeNTC0jPiqb15+5tL4crv3Xn8HnJBHDz3g+g7OCjuzXLripAj72qHHxQ0X8D2kfX4ts90ml7bhvKVz7zBF9/4iInLmO3Ue/0quf8Y7vPQ/XLvjgv4d62aq7of4P4ZsCIYQQB4sCIYQQB4sCIYQQB4sCIYQQB4sCIYQQx6zdR2XV2BHQNYE764evaOfDqvwUXPvRR3E+Ue+wzgQSEUn7+jO7J3WWj4iIV8CT4eLV2FWRGsH5MvdsXqa0F/firKCXjl2F+pbFSajvmD+ixak34NqyJM5EShSxc2jQcJo894LeTnfnOFxbXZmEekcHdjxd7MHncNsxPY3Pdh/dOJ7hKCkZfwrhMyUiJZznc+rkUaXFG3EOUQM22sjEBD7nfkE/K7/++Da49lI3dh8FhnOoZARL+UD2DOdVLIanuoUs+xHYzpTh7ImAiXYiIivqsV678RGoH/6avj6nz52Fa8si+HiyOTwdbu1K/H0z4ulzfu0azqBauBg7A4934Wyqfef18/bBTdgtGcTNu/mfhG8KhBBCHCwKhBBCHCwKhBBCHCwKhBBCHLNuNCfKcWNlsA83OIfHs0oLLcJNm6On8M+6qypwE+6jn9qltPriBFy7Yd46qP/ZUzi6oSHAn7myVdfPvQnc5Kkrww3Yx+/D56osprcz04ObU34C/7z+By/h+t41hJtWyxavUtp4Fm9j8Ro8HCh1Eu9jPouv84H9e5X28cc/CtdaDc5biW8HXUB112O/rrRD7V/GWyjiZvDoZArq85q1tqYRN/B/8KNTUI+G8fEU8lgPhfRxWnEW5eW4c25dNzSsZ3AcP2tf/nYK6n/xO1vwvgzh8/JvP9ymtL8OTcK1b5zCg3CKOFlDgjT+vmlbulZpYyA+RESkolqbV0REQh4wnojIkXa973dv0J8nIlI37w6ozwa+KRBCCHGwKBBCCHGwKBBCCHGwKBBCCHGwKBBCCHHM2n0k1k/jfew0qYhrl0zfOP4peW4aOzM+/zh2OBw/9B2l9ffibbx5FDsTciPYCbR5dw3Uz3doR0A+rQehiIh8+t5KqEtyHpS/9C0dOfFn//E/w7W5Io7QGBvS50REJBC8L9GoHrKTrMbnpDbZAPWaJuw+Gklp55mIyPmzOhZkaBBfn6ZmvX8iIoFxH94MfGPbnjFMKTOjz9fFC+1wbUUiAfWZceyGiYBcjKdewQ6ZM934PrSiG/KCB2Mhp5HlJioWcYxCoYCjKDJZfa7u34qfh0Qcx3N0TfVAffg0dgKd2KsH3kzM4KFbHjb1STjAX5ETKeyYrM1rR1XNQuzeO9R5BepVlXgAWLEQV1pQgx1Z4TCODpoNfFMghBDiYFEghBDiYFEghBDiYFEghBDiYFEghBDimL37yMP1o75lPtTjFbVK6x/CTovly3Dr/8x17KqYymu3wdYHPwXXDg9+G+q72sqgXp/E+/L0Hu2S2bgYOzMaF7RA/T9/9SLUa2q0U6B/+ARc27ZsB9Qf34lzmA71LoB6vqivxelT2DkTiWFX0umT3VCvSeLj7+vTOUznzuthTCK2+2guBNa8l5tkYDpxQg9wmjHcLaEwzr8plfA9fvddeqDOt7/zPFwbCRsOoQBfN0/weuQ08g3nlWd8H+SL2H1UKuhtJ+uwg+nzH8Zut0BAIJSIXM8uh/rfvPWs0rwodmQlQvirMJPH56ptFd6Xyx0DSttQjz+zsxu7ycQ4znRaZ6qlBbvaxBp2NAv4pkAIIcTBokAIIcTBokAIIcTBokAIIcTBokAIIcQxa/fRzDTulFckpqCerNLd71Qed+FrGnDeUPuVYajfvVl33KcuH4drl9bjHJ7JfpxdMpDAzplIZlRpuzYsgWu/t1c7EEREauqwU+uOtTrnJunpnCARkZFr2GXUMYotNWUx7LKaGtHXrX8QX+OfPa2zmUREgiLOpoo34+uMcouOHTsG1+7evRvqtxLP+BvJils6fPio0qKGuyUDnCMiIoGRIbR3/wml9RtTDkNGxlHJyDgKhXC2EHIaWZPXfDClTUQkl8P7WB7X98oLr+Pcq9QUfmb/5Lc3Q31RJXbafP6TW5X23Wd1HpKISKFkuXWwm2p+Ah9/oUI7Cf3GO+Ha6cIh/JElfE/k8vp63or5hHxTIIQQ4mBRIIQQ4mBRIIQQ4mBRIIQQ4mBRIIQQ4pi1+ygo4I54pISzXpIxvekp49MOXtJTzURE1i/FDqGaSr2hk+e74NrxAs7+aQ3j9ReNfXlwk85yGpvBro+T7dits2oFznR5+E7tKIpEtdtJROQHr+g8FxGR4+dxZtPy1bjud4/oKVY5fLolk8UulkgY/4dYFOf8RCJ6H8+fxy6rYhFP9PNvINPlnyLw8D0+PIRdMh0dOisqFtPTsUREJsfxZMCxQexu2Temr08g+LyGQsbEOPPxxscJt2FMXsuk8bUvFfG+hOLaUXTP9la4tr8DO5hOAEeWiEg2jzPV2sL6fqsxcqKuZ/A5qYwZ0+smsZts00ad2TU9hieszYxj52aiCrsxEdb9diPwTYEQQoiDRYEQQoiDRYEQQoiDRYEQQohj1o3mREUj1KsTugErIhICfc9oBNegqQnc/FnRtBDqT+7Rw1123rkGrt21YhX+zAu4kbvvchfUH1yrmz/feAs3pUvGT+YnxnFD7MolfbIOHMYNyLP9uKG8ZvUyqA9fw9EVY/26UeZ5uJHp+7jxVyzhOIJQGF/nsqhuinV2XYJruy6fgvriFRugjuIiQj6Oc7DCAYoF3Pg7dBAPt5mcSCktHscxJDMZ3Jj1wvg6h6L6OueKeBuhMG6G5nK4eWo1j0sl3dy3huyUjCiGKDATiIiEwaV47FEcE1M9dRnrjXiYzld/dB3qP3/+JaVFjOsTy+PrUJvE60vVWM8Ojiut28fnKjuB9Vgc6zNpPZApgXfjhuCbAiGEEAeLAiGEEAeLAiGEEAeLAiGEEAeLAiGEEMfsYy6QnUhE5i3GDqFYQtsNIoYrJSf4J+MX+rAjoDmh3TDLk3ggz7kjP8efmdfDMERE1jRWQb1vWg+r6RjAzp5IFEc0pEvYxfPiuQmllfn4p+7zmvXP6EVE+vvxvoyMYJdVoaCdDMbsFfE9fJsUjQExecPJ4QOXTDqNHVl7Xvgh1D+3DDtQPB8PE4JrjaSMTAbvy4F9r0HdB26tfBZfY0sPRfG5LYGYD89wAkWBq0tEJJfDbirLfYSGIFlrY0b8QyyGHWzpSb0vf/5XJ+Da3zZcSQ9s3Q713VtfhfrVa3o77e2DcC20S4pIxhj4s7IJrx+N1SmtawSvnfb1MygiUg8igkREKiv189bUhM/3jcA3BUIIIQ4WBUIIIQ4WBUIIIQ4WBUIIIQ4WBUIIIY5Zu4+ssRz1DdgNUx7XHfdIBDsZCnns1jl09hrUf/fj9Urb14WdFncvxS6eH73eB/XVjXigzGvntMMjFjZyUQS7CjJpvO1MVg8qak5WwrXTM9qpJCJSMIYg5fP4MxEhw35k59/g64ZcLH+3Ib2+lMPbblmwFG8jwMcJP9OyGRlkprH7qLcXD9nxo+VKK2XwfTi/Ed8rHdewuwU5u+biGvr/rQ+HZ/3YmySM0B37XtH7GMol4NqGFnxOzh3GLiN/AH9P3LtJP0NnzujhRSIifhS7qTwPPxPBFM6hSrToc143gp1nFcYwqqKRKzWvWZ/z+pokXIufzNm9BfBNgRBCiINFgRBCiINFgRBCiINFgRBCiINFgRBCiGPWNgQvwP3s8gjOXaks0531SBl2SQRGJlJ9Urs7RESKeZ1PNNKNJ3g13Isnxi1eqjNKREReeq0L6iMF7RCq8LF7YlKw20Cy2FUQBpOZgoye4CQiEg3jbCbLZGQ5U5AeMaZmFQo4y8jKPrJycUKit1Ms4v1rWbYR6l4Yu7LEcELNhelJfM6zhjsu8PTxlxn3eEsjnlDY1ZeCej6vn4mS8QyGjesWNdwt1nVOg8lelnstEsGunGwWO4eicX08n32iAW8jj++ri2dwjtffvnQc6sUS+A4ycqKKPr5urQtxRtr1ND63oWHtPurswq428fA9YZ3zKMhniljPww34j/imQAghxMGiQAghxMGiQAghxMGiQAghxDHrRrNVPfJF/HPvUlY37cJx3BQJR/BP5qcncMP2uT16oMwD97TBtdkA/9S/rQI3sdd/pBXqz57WjebDZ3BTLeJZzSzcQEJttYyx3+LjJqEYzW2rGYwiEKxYBCv+wtKtzwzAsJ58HjfhRgZwU9EXI+rBu/G/b4YHBqA+ncGN9gKIIyirwY3Jqby+f0TEbpCDeIXAWFsWx/ebFfIxF/OBdS0HBvGwmqzRJK1J6sbs8DDe7+f2nIf6R3ffAfXAw89yCiSOxOL4eHwfmyP8aWw+mFePjSqX+nQMzdudOCZl0SIcwZMxolLqm/T6eHkSrg0CfM8KGAz1/8I3BUIIIQ4WBUIIIQ4WBUIIIQ4WBUIIIQ4WBUIIIY5Zu49KhpUhFMHd7ESFdskkJvHHRQ0Xy+g0dtQgf8PCGF4bGsOfWVuFh7jEKrHbYOatk1o0BnCEIrjzHxRwDUaDSSxnj+UpyeawCyxfwA4pNIDGiqew3CqZDP5Maz0aymO5W86dOwf1hx56COo3g4HBYaiXCvice54+X6UQPvbePuwoCYUMN4g3raRiEd9XVmyFNQTJum7onrOuz+AAdh9Fo3hf8ll9nN//MXYZFafwPZv38H4vaNFDt0RERsb1QJ2KCuwaClsxPlU4yqaQHoH61mU6uuPiJX0tRUSy0/j7oBDGTrWKKh3ZE42VwbU3EvrCNwVCCCEOFgVCCCEOFgVCCCEOFgVCCCEOFgVCCCGO2Q/ZATkvIiI1DSugnmxepLT89T641hqy4xmuivKIzsvxiym49uRJ3PkPezqjRETkucvY4XHxqj5VVkUtWrlFhhyNahdLxHBxGPFEEo/j9ZEI/g/5nPYn+CCb6O+2gR0yVlaS5XpB7pZwGH/myZPA7SUi09P4eiYS2iViuaBEsD4wjDNqjBlDIkXtg4tAb5xIfhrrITA45e83rpTAcMhY7iPr+szMYHcLuhbWObQ+07r1g7w+ifkS3nYIXEsRkb95+grUr09hJ1AsqrdTUZmEa4vDvVBvq8cuwBcu4Jsicf6q0kYzxnNVbuRhFfH6ykqUEWcM9BKcKzUb+KZACCHEwaJACCHEwaJACCHEwaJACCHEwaJACCHEMXv3kTHZKhpNQr2hWbuSPF9PTBMRCYWs3cAZKHe16c56T2E+XLv/yAWo/+bH8IS1qZM4X6U8oTNG0jPYCVMoWI4NfA4jEX38ESMTp7YeT6/b3rYc6pcuajeEiEihpPdlaBA7MDo7dYaMiEihgB1plvtoLtPehoawE2jQmPi1ZMkSsG24VKbGsAuutwtn8eQK2OGBJqGtaMGT165cwNvwDWdXyNPuHt+b2/m2nF2Wjtxh6XQarkVuLxGRUBG7dbLAxZTP4OOJJrCzqXcCu3Wmp/GFrqrU5zafw8/m4DjOPKuN4DywB9ckob6nPaW0Ygg/9yUPO5jChqGoublWbxs44EREwuZ36j8N3xQIIYQ4WBQIIYQ4WBQIIYQ4WBQIIYQ4WBQIIYQ4fvEWtQN3/luadPZRTRWuQaPDOLtkeT12CmzbskBpX/n+dbwfVTVQP3AVuySu9+FMpPJYEuoYa+4R/kzk4onFyuHaBHBBiYj09GCXUeuCeVDPgyyreLnhvKrA+z0+jt1XaJKcpVsT5iYm8HWwJrIh95GVfJRNp6CeGsR6znCDhEL6fMUjeLJXYAQo+SHsbkHnynJqWefKmqRnuY/QlDUr+8jatnU9kUMqn8duIt9wjUWieL+Dydm7/RKRFFy7Yx2+bsNp7ITKpoxJh1ntBsoYww9r4vgZ9yPYUTS/Ue9jKITdiGYIlSH/o8//p5cQQgh5v8CiQAghxMGiQAghxMGiQAghxDGHRrPVtsM01DUrrTKKm1D1Nbjh8uAyHAGw57T+6f3gMP45ft8obvqe6BqAerGIu4q5kB7sI2JEFBjNNqtRGEI/g/fxfpw9g6MYGupww8n3cDPYC+t9qUjon9GLiETCuLmdTOLYEus4UdPSimiw4hXOnDkN9UcffRR8IN52skEbFUREwgl9z4qIhAK8LzFf6zMBbsDmQayIiEjJiK5A95B1X42NjUG9vr4e6tb1yYGOurW2shLfb1VVVVBH1z6Twc1a6zgt3aIY6K+36io0qEbk1+/Fz9uFdhx/8dXXcCRMXbJBaV4Ffn5yAe5AV0fxd21zNYpywdsWD5/b2Qzf4ZsCIYQQB4sCIYQQB4sCIYQQB4sCIYQQB4sCIYQQx+yH7ARG/TB+Nl1X36K0bAF/3KJK/FPyiNFY3/uKHpJSlsRxFsU07sLn8jh2IBLGTigJtNMIuoZk7tEAZWX6QEPGb/3T09itcuFCN96XEB6GsmCedtrkZkbh2lgZ3u/yOL5uvoddIsWidgNZkRjWjdXR0QX1qUntsqqsNGIEfKxPYJORFD0jtiSsr/PoJL6vMgG+bjEfPxORiHaJpI17uWBEaKDYChE75qJU0scTBfshIlJh3BP1dfg5nJ7R7r2JSRzPYe2fdZzWPRT29fEPj+Fz+PTr2GW0deNSqEcXYFffsiY9ZKljBDubCiV8bpMV+DhjZUmgWt9BlgMSyrPYIiGEkPclLAqEEEIcLAqEEEIcLAqEEEIcLAqEEEIcc8g+MvJsDL28QmeA5AQ7EzbMwxkgBy9iOwjKNIn62PETr8BOk3gRW5vyc3A4WO6ORAI7fuLAZSRi5P9Ybi/DaRExnEDDAzjjaXhwWGltS7RzQkQkFMLH2dyM10fjOF+ms0sPVbGcJqEQ1vt6e6A+PKqPp7JqMVybntFrRUTGx7EDxQvhezwMhiklw3hglJWpZQ1kqqzU7p5JY5iMhZUtZOUTxcE9VAKOMRGRkJH7VQEdMiKJMn1uy4H2d+DjtFx9Vj4TemYnrYE3LU1QP3kJX89EHD/jo3l9zvN5/GzGgHtNRKSuDj9XlTXzoY75xeen8U2BEEKIg0WBEEKIg0WBEEKIg0WBEEKIg0WBEEKI4xdvUf89gTHdKg668ysXL4Zr/cx1qJ/sxFaBeEy7W4ol7BqynAmWmypkuGHQassh4xu5RfkcPh7kQfBC2LGQiE5B/UO78dS0xc14QtbBU0NKO36qA66trsYTvJqasV5dh50cBw4cVVplJXYq2blS2Ak1MqwdRUuM+y01obOzRESm09gNE7H+dgr0dU6AvB0RES+P3Ud+HG875muHkO/payYiYg0kixiTDvMF7OqrTuprEY9ix1wRP24yPY4zgaSoz1UkjB1M+QJ+TjwPnysr+ygP8s2yhgusHw1WFJFTF7BTbbqEn/GJNHCkRbUTU8SeOjh/KX5+YmX4Wb7Z8E2BEEKIg0WBEEKIg0WBEEKIg0WBEEKI44YbzWb8Beie7lynB7uIiLy8Fzecpqfxz/TjCR0BECng/SgEeaiXfNyECxk/pS+A8mn09yTIG40v82f6WpvJ4Gbbh7YvgfqmBYNQz2WuQf3x+/XwkKkZHUMhIvLq/itQX5LRg5RERJrz+Fqgxlo0ihvquSxu2NY34mZbsYT3HTE62g/1bA43IcM+3scZ0GydSeN7edECHPEybwGONDhxUjc4PQ/fP5bhoarSaEwa2+nv0/fQqlWr4dq2tmVQDxuRKJFybUoo+fgeT43jaIl0GjexQ0YMSTajG+oFI5pm75ExqIdjOCYnl8b3SrRCN+vLa/A9kc7j65BILId6pKxRaVb0x43ANwVCCCEOFgVCCCEOFgVCCCEOFgVCCCEOFgVCCCGOWbuPwC/6RcTyHmGiZbiTf/4yjm4oM4bVLCvX3fwHRrGrYGI4BfXLHnYlHU1CWcJR/Q8lw8WR8/DP1z3jp/GIYoBzBGIJHMWw/yiObvjp83jIzhd/Sztqdm/BTpiDR/E2rnTheJKhEeweCYf18VvRBcUCvj7zF2BHzcJFet+tSIxcFrvaPMEuo6Lg7RSBmypjxK3E43gbCyqx/ravt+MZeRYh4xym0/i5ikTwdsrL9PM2NjYB107l8DVe3obdce1vHlBaVSU+3+vvWAf1Cxfw/bb/4BGo5/L63JZK+FyVSnhfAuMZ942Ijoq4jgUJhbVbUkREMvj6LF6yAa/3wHWj+4gQQsithEWBEEKIg0WBEEKIg0WBEEKIg0WBEEKI4yZkH2FQTstUaB5ce3XQyACJ4wEsO0t6/ZYGnQsiIpK79x6or39jL9SX5vC0jTdK2rHSk8D7Fy5gd4ttPtKugiU12CGysA6fq//yA5z9cymF3TA/fU1nIv3zJ+6FayOxHqjnxvG5Gh0xhtVEtPssZDhqCiWcc1Mq4OOvqkoqLRB87MlafB9aVjprHwXsy8g0vidGU9g5YxikoFMtEsUulmwaD82pqsLPRCKBnTOr17QpbcECfK6uXML5UaUiPldr1q5SWsjIPgoCrFsZR4mEHkgkgu/DgjFgKBLB58TKmwqH8XHG43o71v1TXY3dfuvWrYX6OwXfFAghhDhYFAghhDhYFAghhDhYFAghhDhYFAghhDhumfuoUNDOj3mLVsC1NTXYPZAZw+6WWpBpEm+tw9tY0gp1398G9XsvdkC94WKX0v46gl1GuYhRa4t4WlNZeVxpUR9fms6rOBNIKvFnVlZiN4wPXBJj6VG4tpTFn1k0nECVlTizKpvR94Tvz+3vkrfePAf1V146pLSPfOSTcG0igR01vnHOpYhdTPmczj46cgpPwJvK4HN1+CJ2Wfk5vT5iuFgyRv5N5xXseNqxYyvUhwZTSmuoxQ6ZBx/aDPW39h+HejSsn/HrfUNwbb6I3UcXLnZDPZfF1wcFttXW4eMpFAwbGHA6iogERsZVLKafKw+NVhSR+a3zoT5vHr4/S2BfjE3fEHxTIIQQ4mBRIIQQ4mBRIIQQ4mBRIIQQ4mBRIIQQ4rhh95HVWX/11ZeVVlObhGs3rcWThva8/hrUJxu102giNQzXFq9ehXrZRuzAKNTUQH31qHaJLJvC7omzUe0mEhEJGRPZysG0pl33NsC1J863Q306iz9zdAznFq1dfIdeO4jzX8amsPvIC+G/KeJx7D6amtbTutJZ7DTJGaP+MjN4WlVfXy/UEfE4ngBYUYGnuo2M4iljvqeziEYm8D3hCXaxXLyKp9olEnpfjGF8JoHxmSOjOJtqwwb9HL6y52249sDhw1CfwWYqCcDEs+k0nupmjXm0Mp6yGZz7lUxot9Zd29bAtW/sPw31MMglExFZuXIh1JvnLVXa8MQYXjsfOyZRfpKISC6nHVLWFL0bgW8KhBBCHCwKhBBCHCwKhBBCHCwKhBBCHLcs5mLePP0T7i996UtwbVWFEYvg40ZZe1br69etg2tr79QNVRGRqavGT+zj+GfwZRuXK23Na7i5eaEMR0t4IdxAqq3WDaeObhw5cb4DNz2vXMTNw/w0/vn+wkX6M4+dwU35QoBvk2QCD32JgoaYiIjn66bYxATeb98abGMMWvG82Xdhy0GsiIhIQyNu7k9M4oZoJKI/M+Tj/SsZE5YKxt9luTxo7gf4nFhmj3gcH+fkJI6POXHipNKmpnA0y+CAEc/h4We5DPT2PSPiJJ0x4mNy2PBQCLBe16CjNYa6cWO/mMUmkEQFPoe//ZtboL53vzZ2jIzj56GpvgXqb775JtS3b98J9ZsN3xQIIYQ4WBQIIYQ4WBQIIYQ4WBQIIYQ4WBQIIYQ4bth9FBgDPlat0j8nb2xohmsPnzoK9YLh2OgIa+fDwJH9cG3o2B6o+x6OOphe0wb1yhoQgVCJnQklbGSQqOEGKSvT7ok1y2vh2v/1t/gn81PT2MXTaMR29AzoyICeUexKkTSOoti9eRPUy6axM+V/nz6lNOsal1k/9S/gW3bNKhxbgohEIlBvNYaeXDWiUnzg+vGBw0pEpGAM6qmIYAdXa31Sad392JFmuYxiMcMdFsV6oaDvw6Kx35EI/kxrZhLaTsmYF1Uy5t0UjAcr8PA57x3QrrGeAD8n0TD+PogK3pnrV/Ewrg892qS0az14G9FKfB26uvD9dt99u5Rmff/eCHxTIIQQ4mBRIIQQ4mBRIIQQ4mBRIIQQ4mBRIIQQ4rgJ2UfYPYKGP2zYpPODREQOHT4C9UIOD9UYrdL5Kj/AES3yyUnsWGjI4AyhyrWroJ4f1x/QX8IOGTEym2JxXIMbKvW5CnLYCTSVwc6esXHsENp5px7gIyLS0qj3/fQP8fleVIZdEpsTOOOpor4V6m9fOq+0zgx2ZoSNPJ+88XdMsrYe6hi8jbV34Jys4yeOQT2d1tciMJ4HCbBbp7kO6/ffpV1j/+upQbi2rBznDVnuI+PUig9ym2Jh7NQqGa6XYoCvpx/SXzWlAl5bymHHUylv3CshvD6X045BL4zXBoK/J/wyfPx+BN9DUxk9BCqawTaraxNXoL5l42NQf6fgmwIhhBAHiwIhhBAHiwIhhBAHiwIhhBAHiwIhhBDHLZu8FgS6m79tG86n2b9XZ+KIiPT2dUM9DzJaOgR3+J+uxe6blVmsN715AOozvt7+OctpEsHbDhvLE3F9rvJF7IYYn9LuBhGRKNg/EZFtm/E0seE+7W6aKeDP3L5oHtQXXr4G9fE27Kj5SOsipX2r9zJca+1LPI7PbTIJsqnmyKLWhVCvrML5UeGo3sdoGb7IZ87jPJtQGOf21NRql0zMyIMKGYFDYcM5hJ5NEZEMcMnkctjWF40azrsA70seOIpyWeyYS+XxNtY0YzfVyrbFUP/xHu3uMcxRUjAyjmor8PVpW4a309+j85YWzcPusJPH8bO8YvkKvPF3CL4pEEIIcbAoEEIIcbAoEEIIcbAoEEIIcdyERjP+ubsHfku/ZvVGuHbxkiVQP3joLajX1FQprboSNwPPpHF0w7kCbnJV+LiBNgMGfJTKjEEjRpPYN6IB6mr1Z46N4ZiLyQncyGytx0NCtqzFzdP//dRZpS2P49iK+1txPInX32foA1BfW6GH2Gwsw822l4dwpEPbUnw8ySS+/ghrLklzI96GnxuC+qaV+pwPpXDcxqnzuKEeGI3ZXE7vJHqmREQiEaMBHcLbLhlTbNKZjNIKBRwLETYazSXj3KYz+nmbTuNr31SJG+S//dENUP/bfV1Qz0X1cUaLeNueYRpJ1mCjxotv4AZ8Xbn+TigK/g4q5fG2GxrxPXTzx+lg+KZACCHEwaJACCHEwaJACCHEwaJACCHEwaJACCHEcetiLkA33/ewY6EcdOxFRPJ57HzIZPXP8aur8c/RwdwQERHJFXE9HAqw6ydWrZ05IcPJIIL3G6RziIjIBHAabbwTu4lam/B+/94T90J9akI7SkREzl0aVtpnmtfBtU27cDxJfjgFdfnxy1hfrH/u/1BLLVy6vx9HaKxdg/cxUam3HVg2I4NKsA0Rkc995m6o18W1++o7T5+Ba8NiuF58497P6nso4uOYh3AIbzscwutzBez68Qr6GYobTqCwj786QvkU1CWi78OQ4GfWi+HP/NZLF6B+4iIemFUR189QEMKfKXnsELp3Oz6Hdz/UBvWBYb2db3ytE6598IProW65zOZ6P/+i8E2BEEKIg0WBEEKIg0WBEEKIg0WBEEKIg0WBEEKI45a5jyQAHXSjq97c3AT1xkasI0ogm0hEpGR07LN5nF1SUYEdKH5OOyKmZvRADRGR8nLsHApFsNNkOq0dDq3NeBvf/suHoF4sYZfET5/Dw11aYo1K22Ece/Y6ziGK7t4BdW8QZwVNTWlnxpohnMNzT20d1O+67x6o++Dvm2AOuVwiIlFjgE8qix+TXHRMadMh7DxrrMTbqKrGlrSKen08UWPITsQY1FMK8P0ZRPF5SbToe665uRmujQ1hB9NjH8X5URMlvY9f+TYeaJWaMp7lnM48ExGJGTlMvq+f2TLrHscmPZnOVUN97z6c+3Xy5KjS6puXwrVbt2Mn3bsN3xQIIYQ4WBQIIYQ4WBQIIYQ4WBQIIYQ4WBQIIYQ4bp37aA584AMfgHpnJ84M+c53vqO0SATnpRSL2N3i+dhusGolziPZtEnn3xw+ehCuPXniJNTr6rDrpXtYO1DGpvFkuFIWO5jOX9ROGBGR1w72Qv23HnhUaYWLOG/IGA4mUSPLqmn5Kqhf+/mLSpsK4W08Ph87Nto24ulbyEtmJVNZGTJFY+relvU7oX7wgM45KgWtcO1aI8sqVkxBfWhE3ytFI8jLE3yv5Iv4ODeswI6aNa3afXXwDP7MwRy+V/aexi64i1f1FZqZwvfyijvwOVyxCE8q6+o2pi6e08678hg+nvE0diM+vycF9dpaPB2tkK/UaxuxU6uhZvbuyncSvikQQghxsCgQQghxsCgQQghxsCgQQghxsCgQQghxvKPuIyufqKkJd+GfeOIJqP/85z9X2sgInr7U2KgzfkREVqxaAvXrvVegfvbcJaVtvHOj8Zn4eCYmUlCPx7WT4cfP4bXDA9hpcqkbu0GWV+N92QaykobqsYsj3IVzXqZPn4V6XRT/rREa0dPexloWwbVBK84+qq3HDpRQCThtjKl7FnAbIlLdgp1QQzMrlZZNvQXXbl2JnTaXrmIXXBqZySwXWAxnH+3ahB/vBx5ogfqrL3co7XQHdq/Nn4+vT88EvoemA52ftWQpXrtwIc44WrlmIdRj5fg4UxPjSstO4myqbSvxviSb8fdHrBw71QYHtIupKoHPd0UC5zC92/BNgRBCiINFgRBCiINFgRBCiINFgRBCiOMdbTRbw00sGhpwU3HBggVKu3oVD5OprcUNpIiHm4ozU7hODgzqZmt3dw9cW57AkQaRKD7dhYJuwB8/hxuQqXHdPBMRmZ7ATfzPG4M8qsoqlDZVwgNfJgd0g1hEJFnE5ypXXwv16MPbldY9iht/ax99EOqRkHHLGtEVc8K4J8IR3BBcsmK10jq7L8C1rcvxPTGUnYZ6cVQ3LMMwzENk0xZ8j3/gXvyZTz19Cuqdw7p5+sijm+Da+lrcDL5j7Va87WvnldZ9FQ9jamjU96aIyLK2LVAfG8dxM/V1ultfimJDyhc/vxzqyQVroP7886ehfuSA/h569OFfhWsrKpJQf7fhmwIhhBAHiwIhhBAHiwIhhBAHiwIhhBAHiwIhhBDHbTFkxxp6EovhgR2LFulohP7+frg2l8OOmmgUDxpZuATXyeEx7T7yjByFi+0Xod7YhN1UaEBQPo/dR4UCjrmorMAxClUT2N2Tb9ZaUJ7Ea2vxucoFM1APJxZDPbRRO1lSR47BtW13YNdUYESlzNXZhjeC70PL17Rxvd5HP3sIrm0/j+MiciU8UGZiultpjdX43rxvG3brvHUMX5+FK/Ggokce26j3I4UHxBRD2JEVq5gP9WSVjrlI103CtVVl+JycOtwF9f5ufI+PDul97xvCbq/n90FZ8qUTUL/eiZ/DVas2K23X7vvhWuuetb4P3yn4pkAIIcTBokAIIcTBokAIIcTBokAIIcTBokAIIcRxW7iP5sonPvEJpVmd/L4+7EoaG8Ouii072qC+fMUqpZ04igfyrFy5AurZHHYUIfdROIwvTcjHdTzw8fF/swsPwvmiX1Ra3Tzs+ugt4H0JT2P3SLyInRkvndyvtO0PYWeGlJdB2TPcRzcHy/WBz21FhR6e0rZgG1y7/82XoJ4axffhylU6t8iLj8K1XRe1s0dE5NW30lDfuRMPq6lp0M6Z/mF8/wxeR1OAROpq8MCfylp9roIovsaH9l6G+vXr+H5rrElC/Z/9uv7Ma6N47cSolXmGnU318/E5/OIf/qHSmpvxkJ3bFb4pEEIIcbAoEEIIcbAoEEIIcbAoEEIIcbAoEEIIcdzW7qOS4TTZulVPd6qqwpOg/uiP/gjqoyPYydHbj50czS0602UmjbNlQsZ0sLVr8RSnfF7nMw0O4v0YNyavNbWAMCMR6c5gB8rPBjqU9oUMdkcl5mH3xPTZTqi/fAzn/zRt0I6NLUYOjxdg14d42N1yUwjmuG1wezYvxVO21m/HDqahN38K9WXrtRNsoojzrbqH8D1RXomddAMj+Dife/UVpS2ajx1pV7uvQX3J4sVQ7x3UE8+uDw7AtTU12L12sR1POhwJY9dYtqifiV/9ID6e6ZHrUH9rD3ZI7b+Avz8utncpbeH8JXDtu51xZME3BUIIIQ4WBUIIIQ4WBUIIIQ4WBUIIIQ4WBUIIIY7b2n00l8lES5cuhWs3bMDulpdffhnq+bzOIRIR6e7Wk7AKeKib1NbWQz0Uwq6Pa9e0kyOXy8G1liNrYesCqB8/ewrqR9M6R+ZUEu/3AiNv6YUhPE3sehVe/1sf/oLS/Bie6hYERsbRTRiwZjO3jYd8sI8edk1tuGM91Lt734J6OKvPYaofn5PVy7FDZmQUTxmTKeya67ygHXy93diVMzCAr/3JEzqzSUQkFNbPz/F9eNuf/QSeurfmUzhT7M3DUJZrvbVKe/VL+Hx7hS6oR2J6yqOIyPAgPoc/+8lPlHb/vTvxtkHmmci770rimwIhhBAHiwIhhBAHiwIhhBAHiwIhhBDHbd1ongvWUJo/+IM/gHpTUxPUjx3DXauuq3qgTiQcg2sXLcLNqclJPCSkvb1dafE4jjRYtmwZ1KeNiIr0FP7M8mSj0t4ew4NTxiuGof5MFkdx/LuHfh/qtQt1szVXwo3ZiDnv5hbGXMwZ0Jg2moTz5s2D+ic+qgdGiYjseflZpfVcx+f7gV/RESwiIktncMTJ0tY6qP/kRX2d778X38uHj+BojZ7BPqh/6tPaCOJFEnDtxR7cgH14t24ci4hsDnBD/RvffFVp13rw0K3Vy1dDfePKTVC/1qsHRomIpMZTSrPMIbcrfFMghBDiYFEghBDiYFEghBDiYFEghBDiYFEghBDi+KVxH1k/DW9s1C4bEZEvfEFHLoiIjIxip82hQ3pwzP59B+DaTAYPCenpwT/rR0N2otEoXFssFqF+uVO7o0REKiproL509XKlHb10Dq4tFPBAlc9+4uNQv/9jn8bbER3H4BuxEGJEnNxOBDAWw3BHGQaU2uSdUN9yl46cOHHib+DavIcjUcTHjqf6lgao33OXjjnxjWFHH/4QHhj1xj78HHZc0ifg4V04mubYMXw8l9txhEZ1PR6cs3CRjtbo78PP96Yta6H+h1/E7sVHHvkVqPeD+I/obRpnYcE3BUIIIQ4WBUIIIQ4WBUIIIQ4WBUIIIQ4WBUIIIQ4vmGUL/HbtlN9srME+iIyRN4QG8oiIHDx4EOoHDmgX07Fjx+Baa/9qkthRssjI3Jma0i6JUHkSrv3jL3wM6ms3PAT1QgQPfQmDWwg7eEippE9WTz92mHVf/THU/9tXnoP62rU492vzeu1I2/v2BFz7gZ3YuHj2PNY7urSL6WOP4QFYW7bg+6qsogXqoRC+h3JZnf30zLP4nNTX4+fn/vvvh7r1fYieTyv7aC7fNTeL2Xwm3xQIIYQ4WBQIIYQ4WBQIIYQ4WBQIIYQ4WBQIIYQ46D66AW6WewC5mL7+9a/DtU8++STUy2I4K2njqgVQv+++O5S2ddfjcG1ji3aliIgExtS0kHFePOg0ovtotpQ8/AxODXdC/cdP/xTq3/omvoeaGvQ9NDCC/25sW1AJ9ft2Pwr1u+7brbRVKxfCtUVjul4QYD0UYMeTDxxcnjGh8VZ+v81127fSlUT3ESGEkDnBokAIIcTBokAIIcTBokAIIcTBRvNtinW+v/nNb0L99MmzUH/iNz8L9Ts3oqEiuAlXMJpTIQ//TRFYjWZwSLj5TBBFY1KPZwz2yedwDMtTP8VRDx3tl5S2ajk2KqxZj4fSLF+B9WhI72NgDPCxGurWcXoBvg9LAhrNpgni1sFGMyGEkPcsLAqEEEIcLAqEEEIcLAqEEEIcLAqEEEIcdB/dpljn29KtQR6RSMTYDlRns2vkPQIa1CMiEgq9838L8vvj9oDuI0IIIXOCRYEQQoiDRYEQQoiDRYEQQoiDRYEQQohj1u4jQgghv/zwTYEQQoiDRYEQQoiDRYEQQoiDRYEQQoiDRYEQQoiDRYEQQoiDRYEQQoiDRYEQQoiDRYEQQojj/wCS0Gxzo8IL1wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Testing the load_data function to ensure everything is correctly extracted\n",
    "index = 1\n",
    "\n",
    "# reshape flattened image back to original shape\n",
    "image = train_x[:, index].reshape(64, 64, 3)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "print(f\"y = {train_y[0, index]} → {classes[train_y[0, index]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d938bf06-4d13-41a5-8b9c-2f1bb905ad78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 224\n",
      "Number of testing examples: 287\n",
      "Each image is of size: (64, 64, 3)\n",
      "Each image in the test dataset is of size: (64, 64, 3)\n",
      "train_x shape: (12288, 224)\n",
      "train_y shape: (1, 224)\n",
      "test_x shape: (12288, 287)\n",
      "test_y shape: (1, 287)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Exploring dataset with flattened data\n",
    "\n",
    "m_train = train_x.shape[1]\n",
    "m_test = test_x.shape[1]\n",
    "num_px = int(np.sqrt(train_x.shape[0] / 3))  # recover image size\n",
    "num_ptx = int(np.sqrt(test_x.shape[0] / 3)) \n",
    "\n",
    "print(\"Number of training examples:\", m_train)\n",
    "print(\"Number of testing examples:\", m_test)\n",
    "print(f\"Each image is of size: ({num_px}, {num_px}, 3)\")\n",
    "pri test datasetnt(f\"Each image in the is of size: ({num_ptx}, {num_ptx}, 3)\")\n",
    "print(\"train_x shape:\", train_x.shape)\n",
    "print(\"train_y shape:\", train_y.shape)\n",
    "print(\"test_x shape:\", test_x.shape)\n",
    "print(\"test_y shape:\", test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1d6f098-1115-4776-93ba-263da774e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [64*64*3, 64, 32, 1]\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) #number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters_deep(layer_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "260a2912-123a-46bf-9c4f-20fbc2f0ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.dot(W,A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4917395-22a0-4db9-86c6-8a76e4abe610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 [[ 1.02978102  1.5651313   1.48469485 ...  0.71428191  0.41198045\n",
      "   1.16310953]\n",
      " [-0.39454563 -0.18174882 -0.439715   ... -0.23873357 -0.50985835\n",
      "  -0.37069164]\n",
      " [-0.0982021   0.26153173  0.40143952 ...  0.21658344  0.23388519\n",
      "   0.61666815]\n",
      " ...\n",
      " [ 0.02824393 -0.06936136  0.09881668 ... -0.15728171 -0.1768648\n",
      "  -0.1340253 ]\n",
      " [ 0.63554338  0.1295908   0.68372902 ...  0.34654998  0.31857561\n",
      "   0.89450654]\n",
      " [ 0.38907601  0.52917551 -0.19612298 ... -0.09288891 -0.13991788\n",
      "   0.13911233]]\n",
      "cache1 (array([[0.5764706 , 0.99607843, 1.        , ..., 0.56078434, 0.6431373 ,\n",
      "        0.9882353 ],\n",
      "       [0.54901963, 0.99607843, 1.        , ..., 0.5019608 , 0.32156864,\n",
      "        0.9882353 ],\n",
      "       [0.7254902 , 0.99607843, 1.        , ..., 0.3882353 , 0.4117647 ,\n",
      "        0.98039216],\n",
      "       ...,\n",
      "       [0.5803922 , 0.99607843, 1.        , ..., 0.16470589, 0.09803922,\n",
      "        0.45490196],\n",
      "       [0.5137255 , 0.99607843, 1.        , ..., 0.13333334, 0.08627451,\n",
      "        0.32941177],\n",
      "       [0.73333335, 0.99607843, 1.        , ..., 0.10196079, 0.06666667,\n",
      "        0.23137255]], dtype=float32), array([[-0.01373644,  0.00509366,  0.00578771, ...,  0.00288929,\n",
      "         0.03311928,  0.00181098],\n",
      "       [-0.01412291,  0.0041385 ,  0.00940258, ...,  0.00654726,\n",
      "         0.00431286, -0.00445158],\n",
      "       [ 0.01309688, -0.00033891, -0.01213686, ...,  0.00680945,\n",
      "        -0.01307244,  0.00393999],\n",
      "       ...,\n",
      "       [ 0.00207065,  0.00392914,  0.00763268, ..., -0.01404437,\n",
      "        -0.01378587,  0.00067991],\n",
      "       [-0.01468962,  0.00105734, -0.00062263, ..., -0.00983768,\n",
      "        -0.00712687,  0.02481023],\n",
      "       [ 0.01706613, -0.01019699, -0.01246245, ..., -0.00883894,\n",
      "         0.00974736, -0.00504665]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]))\n"
     ]
    }
   ],
   "source": [
    "W1 = parameters['W1']  \n",
    "b1 = parameters['b1'] \n",
    "Z1, cache1 = linear_forward(train_x, W1 , b1)\n",
    "print(\"Z1\", Z1)\n",
    "print(\"cache1\", cache1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b48caec8-e52f-4325-a379-16f052b0bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- output of sigmoid(Z), same shape as Z\n",
    "    cache -- Z, stored for backward pass\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0ead1b8-9b68-446e-8a96-22b9a418d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implements the relu activation.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- output of relu(Z), same shape as Z\n",
    "    cache -- Z, stored for backward pass\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ec4a57e9-f89a-4f2b-9b77-25318c88374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "       \n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "       \n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache =  relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9c89efd-0265-4f35-aa97-c9b3faee51a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.02978102, 1.5651313 , 1.48469485, ..., 0.71428191, 0.41198045,\n",
       "         1.16310953],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.26153173, 0.40143952, ..., 0.21658344, 0.23388519,\n",
       "         0.61666815],\n",
       "        ...,\n",
       "        [0.02824393, 0.        , 0.09881668, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.63554338, 0.1295908 , 0.68372902, ..., 0.34654998, 0.31857561,\n",
       "         0.89450654],\n",
       "        [0.38907601, 0.52917551, 0.        , ..., 0.        , 0.        ,\n",
       "         0.13911233]]),\n",
       " ((array([[0.5764706 , 0.99607843, 1.        , ..., 0.56078434, 0.6431373 ,\n",
       "           0.9882353 ],\n",
       "          [0.54901963, 0.99607843, 1.        , ..., 0.5019608 , 0.32156864,\n",
       "           0.9882353 ],\n",
       "          [0.7254902 , 0.99607843, 1.        , ..., 0.3882353 , 0.4117647 ,\n",
       "           0.98039216],\n",
       "          ...,\n",
       "          [0.5803922 , 0.99607843, 1.        , ..., 0.16470589, 0.09803922,\n",
       "           0.45490196],\n",
       "          [0.5137255 , 0.99607843, 1.        , ..., 0.13333334, 0.08627451,\n",
       "           0.32941177],\n",
       "          [0.73333335, 0.99607843, 1.        , ..., 0.10196079, 0.06666667,\n",
       "           0.23137255]], dtype=float32),\n",
       "   array([[-0.01373644,  0.00509366,  0.00578771, ...,  0.00288929,\n",
       "            0.03311928,  0.00181098],\n",
       "          [-0.01412291,  0.0041385 ,  0.00940258, ...,  0.00654726,\n",
       "            0.00431286, -0.00445158],\n",
       "          [ 0.01309688, -0.00033891, -0.01213686, ...,  0.00680945,\n",
       "           -0.01307244,  0.00393999],\n",
       "          ...,\n",
       "          [ 0.00207065,  0.00392914,  0.00763268, ..., -0.01404437,\n",
       "           -0.01378587,  0.00067991],\n",
       "          [-0.01468962,  0.00105734, -0.00062263, ..., -0.00983768,\n",
       "           -0.00712687,  0.02481023],\n",
       "          [ 0.01706613, -0.01019699, -0.01246245, ..., -0.00883894,\n",
       "            0.00974736, -0.00504665]]),\n",
       "   array([[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]])),\n",
       "  array([[ 1.02978102,  1.5651313 ,  1.48469485, ...,  0.71428191,\n",
       "           0.41198045,  1.16310953],\n",
       "         [-0.39454563, -0.18174882, -0.439715  , ..., -0.23873357,\n",
       "          -0.50985835, -0.37069164],\n",
       "         [-0.0982021 ,  0.26153173,  0.40143952, ...,  0.21658344,\n",
       "           0.23388519,  0.61666815],\n",
       "         ...,\n",
       "         [ 0.02824393, -0.06936136,  0.09881668, ..., -0.15728171,\n",
       "          -0.1768648 , -0.1340253 ],\n",
       "         [ 0.63554338,  0.1295908 ,  0.68372902, ...,  0.34654998,\n",
       "           0.31857561,  0.89450654],\n",
       "         [ 0.38907601,  0.52917551, -0.19612298, ..., -0.09288891,\n",
       "          -0.13991788,  0.13911233]])))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_activation_forward(train_x, W1, b1, \"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c7174f6e-0165-4ad6-be52-dd155fdfab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- activation value from the output (last) layer\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    # The for loop starts at 1 because layer 0 is the input\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
    "        \n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\")\n",
    "        \n",
    "    caches.append(cache)\n",
    "          \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4529701-8eb4-49a5-92c8-c82dda29bf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49965906 0.49956928 0.4991316  0.49919252 0.49901956 0.4992267\n",
      "  0.49961935 0.49916704 0.49916058 0.49926374 0.49980497 0.49907368\n",
      "  0.49957475 0.499434   0.49877169 0.4993874  0.49920587 0.49970343\n",
      "  0.49911663 0.49895628 0.49901095 0.49898676 0.49915467 0.49920333\n",
      "  0.49903147 0.49956296 0.49948938 0.49915006 0.49927278 0.49911877\n",
      "  0.49955521 0.49970299 0.49963529 0.49938394 0.49925548 0.49953711\n",
      "  0.49930941 0.49978308 0.49998567 0.49897628 0.49959518 0.49971164\n",
      "  0.49948539 0.49946241 0.49928924 0.49953665 0.49952545 0.49973571\n",
      "  0.49956709 0.49959532 0.49904516 0.49910292 0.49944878 0.49958736\n",
      "  0.49917659 0.4991743  0.49957655 0.49948254 0.49931106 0.49930283\n",
      "  0.49938646 0.49964791 0.49942655 0.49932014 0.49950703 0.49899114\n",
      "  0.49910862 0.49928748 0.49977877 0.49932941 0.49936294 0.49960966\n",
      "  0.49943931 0.49943589 0.4995566  0.49929794 0.49879012 0.49965404\n",
      "  0.49953201 0.49905893 0.49978406 0.49906575 0.49969779 0.49952511\n",
      "  0.4995195  0.49963934 0.49944655 0.49931531 0.49918826 0.49959779\n",
      "  0.49912125 0.49988176 0.49953724 0.49937938 0.49981989 0.4996093\n",
      "  0.49969152 0.49975153 0.49944285 0.49930872 0.49958752 0.49920824\n",
      "  0.49954485 0.4993336  0.49963309 0.49930216 0.49968187 0.49953734\n",
      "  0.49927293 0.49930676 0.49963667 0.49954896 0.49970858 0.49948813\n",
      "  0.49912399 0.49960087 0.49939447 0.49911136 0.49955504 0.49953283\n",
      "  0.49953556 0.49954563 0.49944226 0.49936755 0.49956272 0.49937389\n",
      "  0.49928059 0.49961703 0.49966609 0.49974192 0.49894437 0.49953496\n",
      "  0.49957437 0.49967259 0.49956382 0.49957332 0.49963615 0.49942216\n",
      "  0.49957837 0.49908401 0.49954216 0.49971027 0.49903213 0.49907433\n",
      "  0.49940629 0.49968755 0.49962379 0.49948859 0.49933876 0.49945592\n",
      "  0.49950093 0.49933489 0.49933613 0.49935137 0.49963881 0.49925453\n",
      "  0.49934298 0.49974602 0.49931465 0.49975839 0.49952712 0.49985053\n",
      "  0.49939758 0.49953125 0.49948682 0.49960185 0.4998798  0.49934629\n",
      "  0.49952075 0.4996293  0.49940842 0.49930206 0.49979036 0.4996388\n",
      "  0.499184   0.49926451 0.49964681 0.49982276 0.49961245 0.49963784\n",
      "  0.4993193  0.49946868 0.49961334 0.49935059 0.49927818 0.49944822\n",
      "  0.49970057 0.49976649 0.49977566 0.4997261  0.49953306 0.49943609\n",
      "  0.50016363 0.49939921 0.49908439 0.49922182 0.49922927 0.49977993\n",
      "  0.49956713 0.4990336  0.49952549 0.49952396 0.49977469 0.49970843\n",
      "  0.49968393 0.49922492 0.49955922 0.49923678 0.49962506 0.49965439\n",
      "  0.49934824 0.49968971 0.49919196 0.49940743 0.49988494 0.49935976\n",
      "  0.4995466  0.49920191 0.49965596 0.49916312 0.49956797 0.49952027\n",
      "  0.49971042 0.4993826 ]]\n"
     ]
    }
   ],
   "source": [
    "#In our case X = A0.shape\n",
    "AL , caches = L_model_forward(train_x, parameters)\n",
    "print(AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba52ccef-a48e-443f-8f1c-77c7d129ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined \n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-face , 1 if face), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (-1/m) * np.sum((Y * np.log(AL)) + ((1-Y) * np.log(1-AL)))    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d9d46fb-c9a5-4f4e-b025-49df6ab3eb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.693236898944617\n"
     ]
    }
   ],
   "source": [
    "Y = train_y.reshape(1, -1) #to make sure the size is consistent with AL\n",
    "\n",
    "cost = compute_cost(AL, Y)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf80417c-4815-4d77-97e1-b9d53fa98ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m) * np.dot(dZ ,A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ) \n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dcc4483b-1850-412c-a36e-d579871b4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a ReLU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    activation_cache -- 'Z' stored during forward propagation\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA, copy=True)  # just to avoid modifying dA directly\n",
    "\n",
    "    # Zero out gradients where Z was negative\n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f453a844-e76d-42f0-abfd-717764cb65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a sigmoid unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    activation_cache -- 'Z' stored during forward propagation\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    # Compute the sigmoid of Z\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d47ef5eb-b2fb-45d2-9a4c-544b433ba19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        \n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "      \n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        \n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6fa5984e-fbcf-4513-8e63-31aa59fd42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    " \n",
    "    current_cache = caches[L - 1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
    "    grads[\"dA\" + str(L - 1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "      \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cb9d47e8-ca24-4feb-bd3a-77563dca6f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = copy.deepcopy(params)\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        \n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5c145583-e0e9-47eb-a47c-d8dd7d9cfb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "   \n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                        \n",
    "        # Print the cost every 100 iterations and for the last iteration\n",
    "        #if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "        #    print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "       # if i % 100 == 0:\n",
    "         #   costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "42371cc5-1cee-4286-8149-b26625f4492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the labels for a dataset X using learned parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of size (n_x, number of examples)\n",
    "    Y -- true labels of size (1, number of examples)\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2  # number of layers\n",
    "    p = np.zeros((1, m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    AL, _ = L_model_forward(X, parameters)\n",
    "    \n",
    "    # Convert probabilities to 0/1 predictions\n",
    "    #for i in range(AL.shape[1]):\n",
    "     #   if AL[0, i] > 0.5:\n",
    "    #        p[0, i] = 1\n",
    "    #    else:\n",
    "    #        p[0, i] = 0\n",
    "\n",
    "    p = (AL > 0.5).astype(int)\n",
    "    accuracy = np.mean(p == Y)  # element-wise comparison, then average\n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "   \n",
    "    \n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "88bf2fe1-2ee5-41da-8dbc-a86c0c5941b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [64*64*3, 64, 32, 1]\n",
    "parameters, costs = L_layer_model(train_x, train_y, layer_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd3ef7b7-2af6-41ee-887a-b951afe4ef5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_x,train_y , parameters)\n",
    "print(pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f219de-e89c-4a3f-a496-398dd12e2829",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = []\n",
    "parameters , costs = L_layer_model(train_x, train_y, layer_dims, num_iterations = 2500, print_cost = True)\n",
    "pred_test = predict(test_x, test_y, parameters)\n",
    "print(pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
